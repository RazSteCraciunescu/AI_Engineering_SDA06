{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KpMqS5eNEu9"
      },
      "source": [
        "---\n",
        "# Tehnici de Optimizare pentru Seturi Mari de Date\n",
        "\n",
        "Când lucrăm cu seturi de date mici, performanța nu este o problemă. Însă, în lumea reală, seturile de date pot avea milioane sau chiar miliarde de rânduri. În astfel de cazuri, memoria RAM și timpul de procesare devin resurse critice. Acest capitol se concentrează pe tehnici esențiale pentru a lucra eficient cu date de mari dimensiuni în Pandas, asigurându-ne că codul nostru rulează rapid și fără a epuiza memoria calculatorului."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PbNKaN9pNEu7",
        "outputId": "6be033ac-e8ac-4345-ad04-ae7f0ea74aac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame-ul de angajați:\n",
            "             Nume Departament  Salariu_Brut_EUR  Vechime_ani\n",
            "0  Andrei Popescu          IT              3000            5\n",
            "1   Maria Ionescu     Vânzări              2500            3\n",
            "2    Ion Gheorghe          IT              2800            4\n",
            "3    Elena Vasile   Marketing              2200            2\n",
            "4   Vasile Costin     Vânzări              2600            4\n",
            "\n",
            "DataFrame-ul de produse:\n",
            "      Produs     Categorie  Pret_RON  Stoc\n",
            "0     Tricou  Îmbrăcăminte      80.0   100\n",
            "1  Pantaloni  Îmbrăcăminte     250.0    50\n",
            "2    Adidași  Încălțăminte     400.0    75\n",
            "3      Șapcă     Accesorii      50.0   200\n",
            "4      Geacă  Îmbrăcăminte     500.0    25\n",
            "5    Hanorac  Îmbrăcăminte       NaN    40\n",
            "\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 6 entries, 0 to 5\n",
            "Data columns (total 4 columns):\n",
            " #   Column     Non-Null Count  Dtype  \n",
            "---  ------     --------------  -----  \n",
            " 0   Produs     6 non-null      object \n",
            " 1   Categorie  6 non-null      object \n",
            " 2   Pret_RON   5 non-null      float64\n",
            " 3   Stoc       6 non-null      int64  \n",
            "dtypes: float64(1), int64(1), object(2)\n",
            "memory usage: 324.0+ bytes\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "# Exemplu: Crearea unui DataFrame pentru exemplele noastre\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "date_angajati = {\n",
        "    'Nume': ['Andrei Popescu', 'Maria Ionescu', 'Ion Gheorghe', 'Elena Vasile', 'Vasile Costin'],\n",
        "    'Departament': ['IT', 'Vânzări', 'IT', 'Marketing', 'Vânzări'],\n",
        "    'Salariu_Brut_EUR': [3000, 2500, 2800, 2200, 2600],\n",
        "    'Vechime_ani': [5, 3, 4, 2, 4]\n",
        "}\n",
        "\n",
        "df_angajati = pd.DataFrame(date_angajati)\n",
        "\n",
        "print(\"DataFrame-ul de angajați:\")\n",
        "print(df_angajati)\n",
        "\n",
        "print()\n",
        "\n",
        "date_produse = {\n",
        "    'Produs': ['Tricou', 'Pantaloni', 'Adidași', 'Șapcă', 'Geacă', 'Hanorac'],\n",
        "    'Categorie': ['Îmbrăcăminte', 'Îmbrăcăminte', 'Încălțăminte', 'Accesorii', 'Îmbrăcăminte', 'Îmbrăcăminte'],\n",
        "    'Pret_RON': [80, 250, 400, 50, 500, np.nan],\n",
        "    'Stoc': [100, 50, 75, 200, 25, 40]\n",
        "}\n",
        "\n",
        "df_produse = pd.DataFrame(date_produse)\n",
        "\n",
        "print(\"DataFrame-ul de produse:\")\n",
        "print(df_produse)\n",
        "print()\n",
        "print(df_produse.info())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCLz72dyNEu9"
      },
      "source": [
        "## Folosirea Tipurilor de Date Eficiente\n",
        "\n",
        "Când Pandas încarcă date, alocă automat tipuri de date pentru fiecare coloană. De exemplu, pentru numere întregi folosește `int64`, iar pentru cele raționale `float64`. Aceste tipuri de date folosesc 64 de biți pentru a stoca fiecare valoare, ceea ce poate fi un consum inutil de memorie dacă valorile din coloană sunt mici.\n",
        "\n",
        "Putem reduce semnificativ consumul de memorie prin conversia coloanelor la tipuri de date mai mici (ex: `int32`, `float32`) sau la tipuri specializate, cum ar fi **`category`**.\n",
        "\n",
        "Tipul `category` este ideal pentru coloanele care au un număr redus de valori unice (de exemplu, 'Departament', 'Țară', 'Status'). În loc să stocheze textul de nenumărate ori, Pandas stochează doar valorile unice și apoi folosește numere întregi (coduri) pentru a face referire la ele, economisind astfel foarte mult spațiu."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NrJ3uMXXNEu-",
        "outputId": "54bb9a46-36ca-4e0e-bde3-3f02cf5da484"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Consumul de memorie inițial:\n",
            "Index               132\n",
            "Nume                309\n",
            "Departament         340\n",
            "Salariu_Brut_EUR     40\n",
            "Vechime_ani          40\n",
            "dtype: int64\n",
            "\n",
            "Consumul de memorie după optimizare:\n",
            "Index               132\n",
            "Nume                309\n",
            "Departament         312\n",
            "Salariu_Brut_EUR     20\n",
            "Vechime_ani           5\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Exemplu 1: Verificarea consumului de memorie și optimizarea tipurilor de date\n",
        "\n",
        "print(\"Consumul de memorie inițial:\")\n",
        "print(df_angajati.memory_usage(deep=True))\n",
        "\n",
        "# Acum, să optimizăm tipurile de date.\n",
        "df_optim = df_angajati.copy()\n",
        "\n",
        "# Coloana 'Departament' este un candidat perfect pentru tipul 'category'.\n",
        "df_optim['Departament'] = df_optim['Departament'].astype('category')\n",
        "\n",
        "# 0 IT\n",
        "# 1 IT\n",
        "# 2 Marketing\n",
        "# 3 Marketing\n",
        "# 4 Marketing\n",
        "# 5 Marketing\n",
        "\n",
        "# -> IT Marketing\n",
        "# -> IT > 0 | Marketing > 1\n",
        "\n",
        "# 0 0\n",
        "# 1 0\n",
        "# 2 1\n",
        "# 3 1\n",
        "# 4 1\n",
        "# 5 1\n",
        "\n",
        "\n",
        "# Salariile pot fi stocate ca float32, iar vechimea ca int8 (un întreg pe 8\n",
        "# biți, -128 la 127).\n",
        "df_optim['Salariu_Brut_EUR'] = df_optim['Salariu_Brut_EUR'].astype('float32')\n",
        "df_optim['Vechime_ani'] = df_optim['Vechime_ani'].astype('int8')\n",
        "\n",
        "print(\"\\nConsumul de memorie după optimizare:\")\n",
        "print(df_optim.memory_usage(deep=True))\n",
        "\n",
        "# OBS.: `deep=True` asigură calcularea exactă a memoriei ocupate de obiectele\n",
        "# de tip string, nu doar de referințele către ele.\n",
        "# Reducerea memoriei poate fi dramatică pe seturi de date mari."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "brJvuMpENEu-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39e04be1-ef66-4d1e-927d-3d1c890af9c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index        132\n",
            "Produs       401\n",
            "Categorie    508\n",
            "Pret_RON      48\n",
            "Stoc          48\n",
            "dtype: int64\n",
            "\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 6 entries, 0 to 5\n",
            "Data columns (total 4 columns):\n",
            " #   Column     Non-Null Count  Dtype  \n",
            "---  ------     --------------  -----  \n",
            " 0   Produs     6 non-null      object \n",
            " 1   Categorie  6 non-null      object \n",
            " 2   Pret_RON   5 non-null      float64\n",
            " 3   Stoc       6 non-null      int64  \n",
            "dtypes: float64(1), int64(1), object(2)\n",
            "memory usage: 324.0+ bytes\n",
            "None\n",
            "\n",
            "Index        132\n",
            "Produs       401\n",
            "Categorie    385\n",
            "Pret_RON      12\n",
            "Stoc          24\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# __EXERCIȚIU__\n",
        "# Creeați un DataFrame nou pe baza DataFrame-ului `df_produse`. Optimizați\n",
        "# spațiul de stocare alocat setului de date schimbând tipul de date al unora\n",
        "# dintre coloane.\n",
        "\n",
        "print(df_produse.memory_usage(deep=True))\n",
        "print()\n",
        "print(df_produse.info())\n",
        "print()\n",
        "\n",
        "df_copie = df_produse.copy()\n",
        "\n",
        "df_copie[\"Categorie\"] = df_copie[\"Categorie\"].astype(\"category\")\n",
        "df_copie[\"Pret_RON\"] = df_copie[\"Pret_RON\"].astype(\"float16\")\n",
        "df_copie[\"Stoc\"] = df_copie[\"Stoc\"].astype(\"int32\")\n",
        "\n",
        "print(df_copie.memory_usage(deep=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07VDYLaaNEu-"
      },
      "source": [
        "## Procesarea în Bucăți (Chunk Processing)\n",
        "\n",
        "Ce se întâmplă când un fișier este atât de mare încât nu încape deloc în memoria RAM? Răspunsul este procesarea în bucăți (**chunks**). În loc să citim tot fișierul deodată, îl citim pe bucăți de dimensiuni fixe (de exemplu, 10.000 de rânduri odată).\n",
        "\n",
        "Acest lucru se realizează folosind parametrul `chunksize` din funcția `pd.read_csv()`. Aceasta nu mai returnează un DataFrame, ci un iterator, pe care putem să-l parcurgem cu o buclă `for`, procesând fiecare bucată individual. Această tehnică este esențială pentru a putea lucra cu seturi de date care depășesc capacitatea memoriei RAM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7dBewJ4NEu-",
        "outputId": "a66c8a52-dd82-4acb-ae56-38b269abd6cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- O nouă bucată a fost procesată ---\n",
            "             Nume Departament  Salariu_Brut_EUR  Vechime_ani\n",
            "0  Andrei Popescu          IT              3000            5\n",
            "1   Maria Ionescu     Vânzări              2500            3\n",
            "--- O nouă bucată a fost procesată ---\n",
            "           Nume Departament  Salariu_Brut_EUR  Vechime_ani\n",
            "2  Ion Gheorghe          IT              2800            4\n",
            "3  Elena Vasile   Marketing              2200            2\n",
            "--- O nouă bucată a fost procesată ---\n",
            "            Nume Departament  Salariu_Brut_EUR  Vechime_ani\n",
            "4  Vasile Costin     Vânzări              2600            4\n",
            "\n",
            "Salariul mediu calculat prin procesarea în bucăți: 2620.00 EUR\n"
          ]
        }
      ],
      "source": [
        "# Exemplu: Simularea citirii unui fișier mare în bucăți\n",
        "\n",
        "# Mai întâi, vom crea un fișier CSV demonstrativ.\n",
        "df_angajati.to_csv('angajati.csv', index=False)\n",
        "\n",
        "suma_salarii_totala = 0\n",
        "numar_randuri_total = 0\n",
        "dimensiune_bucata = 2  # Folosim o dimensiune mică pentru a demonstra conceptul\n",
        "\n",
        "# Citim fișierul CSV în bucăți de câte 2 rânduri\n",
        "for bucata in pd.read_csv('angajati.csv', chunksize=dimensiune_bucata):\n",
        "    print(\"--- O nouă bucată a fost procesată ---\")\n",
        "    print(bucata)\n",
        "\n",
        "    # Aplicăm o operație pe bucată: calculăm suma salariilor\n",
        "    suma_salarii_totala += bucata['Salariu_Brut_EUR'].sum()\n",
        "    numar_randuri_total += len(bucata)\n",
        "\n",
        "salariu_mediu_calculat = suma_salarii_totala / numar_randuri_total\n",
        "\n",
        "print(f\"\\nSalariul mediu calculat prin procesarea în bucăți: {salariu_mediu_calculat:.2f} EUR\")\n",
        "\n",
        "# OBS.: Această tehnică ne permite să calculăm statistici agregate (sumă, medie,\n",
        "# etc.) pe un fișier întreg,\n",
        "# fără a-l încărca complet în memorie. Păstrăm doar rezultatele intermediare\n",
        "# (suma și numărul de rânduri)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "1ICu55goNEu-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "969b4e14-ff9b-407f-988a-7d6e4f514c14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Valoarea maxima locala este: 5\n",
            "Noul maxim a devenit: 5\n",
            "\n",
            "Valoarea maxima locala este: 4\n",
            "\n",
            "Cea mai mare vechime este de 5 ani.\n"
          ]
        }
      ],
      "source": [
        "# __EXERCIȚIU__\n",
        "# Folosind fișierul 'angajati.csv' și procesarea în bucăți, găsiți cea mai mare\n",
        "# vechime în ani din întregul set de date.\n",
        "# La final, afișați un mesaj: \"Cea mai mare vechime este de [...] ani.\"\n",
        "\n",
        "valoarea_maxima_globala = 0\n",
        "\n",
        "dimensiune_bucata = 3\n",
        "\n",
        "for bucata in pd.read_csv('angajati.csv', chunksize=dimensiune_bucata):\n",
        "  valoarea_maxima_locala = bucata[\"Vechime_ani\"].max()\n",
        "  print(f\"Valoarea maxima locala este: {valoarea_maxima_locala}\")\n",
        "\n",
        "  if valoarea_maxima_locala > valoarea_maxima_globala:\n",
        "    valoarea_maxima_globala = valoarea_maxima_locala\n",
        "    print(f\"Noul maxim a devenit: {valoarea_maxima_globala}\")\n",
        "\n",
        "  print()\n",
        "\n",
        "print(f\"Cea mai mare vechime este de {valoarea_maxima_globala} ani.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTP1y2CgNEu-"
      },
      "source": [
        "---\n",
        "# Capitolul: Lucrul cu Fișiere Mari și Formate Eficiente\n",
        "\n",
        "Continuăm explorarea tehnicilor pentru date mari, concentrându-ne de data aceasta pe vizualizarea progresului operațiunilor de lungă durată și pe utilizarea unor formate de fișiere optimizate pentru viteză și spațiu. Când o operație durează minute sau chiar ore, este esențial să avem o idee despre stadiul ei. De asemenea, formatul în care salvăm datele poate avea un impact uriaș asupra performanței."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxC3FdOwNEu-"
      },
      "source": [
        "## Bare de Progres cu `tqdm`\n",
        "\n",
        "Când procesăm un fișier mare în bucăți, poate dura mult timp, iar programul pare că a \"înghețat\". Pentru a oferi feedback vizual, putem folosi biblioteca **`tqdm`**. Aceasta se integrează foarte ușor cu buclele din Python și afișează o bară de progres inteligentă, care ne arată procentul de finalizare, timpul scurs și o estimare a timpului rămas.\n",
        "\n",
        "! Dacă nu aveți `tqdm` instalat, rulați comanda `pip install tqdm` în terminal sau `!pip install tqdm` într-o celulă de cod."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3YnJKWpNEu-",
        "outputId": "f16f730f-952e-4182-8127-ed27b6dd7b31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Procesare angajați: 100%|██████████| 2/2 [00:02<00:00,  1.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "DataFrame-ul a fost reconstruit din bucăți.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Exemplu: Adăugarea unei bare de progres la procesarea în bucăți\n",
        "\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "\n",
        "# Pentru ca tqdm să poată afișa progresul total, trebuie să știe numărul total\n",
        "# de iterații. Vom calcula numărul total de bucăți:\n",
        "# total_randuri / dimensiune_bucata\n",
        "\n",
        "total_randuri = len(df_angajati)\n",
        "dimensiune_bucata = 3\n",
        "numar_bucati = math.ceil(total_randuri / dimensiune_bucata)\n",
        "print(numar_bucati)\n",
        "rezultate = []\n",
        "reader = pd.read_csv('angajati.csv', chunksize=dimensiune_bucata)\n",
        "import time\n",
        "\n",
        "# Împachetăm iteratorul nostru în tqdm()\n",
        "for bucata in tqdm(reader, total=numar_bucati, desc=\"Procesare angajați\"):\n",
        "    # Aici ar avea loc o procesare complexă\n",
        "    # Pentru exemplu, doar adăugăm bucata la o listă\n",
        "    rezultate.append(bucata)\n",
        "    time.sleep(1)\n",
        "\n",
        "df_final = pd.concat(rezultate)\n",
        "print(\"\\nDataFrame-ul a fost reconstruit din bucăți.\")\n",
        "\n",
        "# OBS.: Am folosit `total=numar_bucati` pentru a-i spune lui tqdm câte\n",
        "# iterații vor fi în total. `desc` este un text descriptiv afișat în fața barei\n",
        "# de progres."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "4jk4YhPeNEu-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3669dc9b-88d9-49a4-c243-9ef369326bb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Calcul Vechime Maximă..................: 100%|██████████| 2/2 [00:02<00:00,  1.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Cea mai mare vechime este de 5 ani.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# __EXERCIȚIU__\n",
        "# Repetați exercițiul anterior (găsirea vechimii maxime folosind chunk\n",
        "# processing), dar de data aceasta adăugați o bară de progres cu `tqdm` pentru\n",
        "# a vizualiza procesul.\n",
        "\n",
        "# HINT: Calculați `numar_bucati` și pasați iteratorul `pd.read_csv()` prin\n",
        "# `tqdm()`, specificând `total` și un `desc`.\n",
        "\n",
        "\n",
        "valoarea_maxima_globala = 0\n",
        "\n",
        "dimensiune_bucata = 3\n",
        "total_randuri = len(df_angajati)\n",
        "numar_bucati = math.ceil(total_randuri / dimensiune_bucata)\n",
        "\n",
        "reader = pd.read_csv('angajati.csv', chunksize=dimensiune_bucata)\n",
        "\n",
        "import time\n",
        "\n",
        "for bucata in tqdm(reader, total = numar_bucati, desc = \"Calcul Vechime Maximă\"):\n",
        "  valoarea_maxima_locala = bucata[\"Vechime_ani\"].max()\n",
        "  #print(f\"Valoarea maxima locala este: {valoarea_maxima_locala}\")\n",
        "\n",
        "  if valoarea_maxima_locala > valoarea_maxima_globala:\n",
        "    valoarea_maxima_globala = valoarea_maxima_locala\n",
        "    #print(f\"Noul maxim a devenit: {valoarea_maxima_globala}\")\n",
        "\n",
        "  #print()\n",
        "  time.sleep(1)\n",
        "\n",
        "print()\n",
        "print(f\"Cea mai mare vechime este de {valoarea_maxima_globala} ani.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHhTOKjUNEu-"
      },
      "source": [
        "## Formate de Fișiere Eficiente: Parquet\n",
        "\n",
        "Fișierele `.csv` sunt universale, dar nu sunt deloc eficiente. Ele stochează datele ca text, ocupă mult spațiu și sunt lente la citire. Pentru seturi de date mari, se folosesc formate de fișiere specializate.\n",
        "\n",
        "**Parquet** este un format de fișier **columnar**, ceea ce înseamnă că stochează datele pe coloane, nu pe rânduri. Această abordare are avantaje uriașe:\n",
        "\n",
        "* **Compresie mai bună**: Deoarece datele de pe o coloană sunt de același tip, pot fi comprimate mult mai eficient. Fișierele Parquet sunt adesea de câteva ori mai mici decât echivalentele lor CSV.\n",
        "* **Citire mai rapidă**: Când realizăm o interogare, adesea avem nevoie doar de câteva coloane. Un format columnar permite citirea doar a acelor coloane, ignorând restul, ceea ce duce la o viteză de citire mult mai mare.\n",
        "* **Păstrarea tipurilor de date**: Spre deosebire de CSV, Parquet salvează și tipurile de date, deci nu mai este nevoie să le specificăm la citire.\n",
        "\n",
        "! Pentru a lucra cu Parquet, trebuie să instalați o bibliotecă precum `pyarrow`: `pip install pyarrow`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Dx5hHGXQNEu_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb23e58b-de09-402a-f370-27311137257e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tipuri de date înainte de salvare:\n",
            "Nume                  object\n",
            "Departament         category\n",
            "Salariu_Brut_EUR     float32\n",
            "Vechime_ani             int8\n",
            "dtype: object\n",
            "\n",
            "DataFrame-ul a fost salvat ca 'angajati.parquet'.\n",
            "\n",
            "Tipuri de date după citirea din Parquet:\n",
            "Nume                  object\n",
            "Departament         category\n",
            "Salariu_Brut_EUR     float32\n",
            "Vechime_ani             int8\n",
            "dtype: object\n"
          ]
        }
      ],
      "source": [
        "# Exemplu: Salvarea și citirea unui fișier Parquet\n",
        "\n",
        "# Să folosim DataFrame-ul nostru optimizat, df_optim\n",
        "print(\"Tipuri de date înainte de salvare:\")\n",
        "print(df_optim.dtypes)\n",
        "\n",
        "# Salvarea în format Parquet\n",
        "df_optim.to_parquet('angajati.parquet', engine='pyarrow')\n",
        "\n",
        "print(\"\\nDataFrame-ul a fost salvat ca 'angajati.parquet'.\")\n",
        "\n",
        "# Citirea din formatul Parquet\n",
        "df_citit_parquet = pd.read_parquet('angajati.parquet', engine='pyarrow')\n",
        "\n",
        "print(\"\\nTipuri de date după citirea din Parquet:\")\n",
        "print(df_citit_parquet.dtypes)\n",
        "\n",
        "# OBS.: Observați cum tipurile de date optimizate ('category', 'float32',\n",
        "# 'int8') au fost păstrate automat la citirea fișierului Parquet, un avantaj\n",
        "# major față de CSV."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74jUm1MSNEu_"
      },
      "outputs": [],
      "source": [
        "# __EXERCIȚIU__\n",
        "# 1. Creați un nou DataFrame numit `df_vanzari` cu următoarele date:\n",
        "#    - 'Produs': ['Laptop', 'Mouse', 'Tastatura', 'Monitor', 'Laptop']\n",
        "#    - 'Pret': [5500.50, 150.75, 250.00, 1200.50, 5700.00]\n",
        "#    - 'Cantitate': [10, 50, 30, 15, 8]\n",
        "# 2. Optimizați tipurile de date: 'Produs' -> 'category', 'Pret' -> 'float32',\n",
        "# 'Cantitate' -> 'int16'.\n",
        "# 3. Salvați DataFrame-ul optimizat într-un fișier numit 'vanzari.parquet'.\n",
        "# 4. Citiți datele înapoi din 'vanzari.parquet' într-un nou DataFrame și\n",
        "# afișați-l.\n",
        "\n",
        "# HINT: Urmați pașii din exemplul anterior: creați DataFrame-ul, folosiți\n",
        "# .astype() pentru fiecare coloană, apoi .to_parquet() și .read_parquet()."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjwrMioP6BZ0"
      },
      "source": [
        "---\n",
        "# Lucrul cu Arhive și Formate de Date Complexe\n",
        "\n",
        "Pe măsură ce seturile de date devin mai mari, gestionarea fișierelor devine o provocare. Trimiterea unui fișier CSV de 2GB prin email este impracticabilă. Din acest motiv, datele sunt adesea arhivate pentru a reduce spațiul pe disc și pentru a facilita transferul. În acest capitol vom învăța cum să lucrăm direct cu fișiere arhivate și vom explora formate de date avansate, optimizate pentru performanță."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTzfnB5q6BZ0"
      },
      "source": [
        "## Lucrul cu fișiere arhivate (.zip, .tar.gz)\n",
        "\n",
        "Arhivarea fișierelor (cum ar fi în formate `.zip` sau `.gzip`) este o practică standard pentru a reduce dimensiunea datelor și pentru a grupa mai multe fișiere într-unul singur. Vestea bună este că Pandas poate citi direct din aceste arhive, fără a fi nevoie să le dezarhivăm manual.\n",
        "\n",
        "Pentru formate precum `.zip` și `.gzip`, funcția `pd.read_csv()` poate gestiona direct calea către arhivă. Pentru arhive mai complexe precum `.tar`, am avea nevoie de o bibliotecă suplimentară precum `tarfile` pentru a extrage fișierul în memorie înainte de a-l citi cu Pandas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "4NKVej_j6BZ0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "536edfbf-991b-4e77-9ec2-ae6ca0e3c348"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Datele citite din arhiva 'produse.zip':\n",
            "      Produs     Categorie  Pret_RON  Stoc\n",
            "0     Tricou  Îmbrăcăminte      80.0   100\n",
            "1  Pantaloni  Îmbrăcăminte     250.0    50\n",
            "2    Adidași  Încălțăminte     400.0    75\n",
            "3      Șapcă     Accesorii      50.0   200\n",
            "4      Geacă  Îmbrăcăminte     500.0    25\n",
            "5    Hanorac  Îmbrăcăminte       NaN    40\n"
          ]
        }
      ],
      "source": [
        "# Exemplu 1: Citirea directă dintr-un fișier .zip\n",
        "\n",
        "# Mai întâi, vom crea un fișier .zip pentru demonstrație.\n",
        "df_produse.to_csv('produse.csv', index=False)\n",
        "\n",
        "import zipfile\n",
        "with zipfile.ZipFile('produse.zip', 'w') as zf:\n",
        "    zf.write('produse.csv')\n",
        "\n",
        "# Acum, citim direct din arhiva .zip\n",
        "df_din_zip = pd.read_csv('produse.zip')\n",
        "\n",
        "print(\"Datele citite din arhiva 'produse.zip':\")\n",
        "print(df_din_zip)\n",
        "\n",
        "# OBS.: Pandas a decomprimat automat fișierul în memorie și l-a citit. Nu a\n",
        "# fost nevoie de niciun pas manual de dezarhivare."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E1Cbe2k-6BZ0"
      },
      "outputs": [],
      "source": [
        "# __EXERCIȚIU__\n",
        "# Scrieți codul necesar pentru a citi un fișier CSV numit 'date_clienti.csv'\n",
        "# dintr-o arhivă numită 'clienti_arhiva.zip'.\n",
        "# Stocați rezultatul într-un DataFrame numit `df_clienti`.\n",
        "\n",
        "# HINT: Sintaxa este identică cu cea pentru un fișier CSV normal, doar calea\n",
        "# se schimbă: `pd.read_csv('clienti_arhiva.zip')`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fkE9iuH6BZ0"
      },
      "source": [
        "## Formatul HDF5 (Hierarchical Data Format)\n",
        "\n",
        "**HDF5** este un format de fișier și o bibliotecă concepute pentru a stoca și a organiza cantități mari de date. Spre deosebire de formatele simple precum CSV, HDF5 este un sistem de fișiere în sine, capabil să stocheze seturi de date multiple într-o structură ierarhică, similară cu folderele și fișierele de pe un computer. Este foarte eficient pentru seturi de date mari și complexe și a fost utilizat pe scară largă pentru stocarea modelelor în biblioteci de machine learning precum Keras.\n",
        "\n",
        "! Pentru a lucra cu HDF5 în Pandas, trebuie instalată biblioteca `tables`: `pip install tables`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "ub9PuCfc6BZ0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b87458e-042c-4c87-9bd1-57adccacc3f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame-ul a fost salvat în 'magazin_date.h5' sub cheia 'produse'.\n",
            "\n",
            "Datele citite din HDF5:\n",
            "      Produs     Categorie  Pret_RON  Stoc\n",
            "0     Tricou  Îmbrăcăminte      80.0   100\n",
            "1  Pantaloni  Îmbrăcăminte     250.0    50\n",
            "2    Adidași  Încălțăminte     400.0    75\n",
            "3      Șapcă     Accesorii      50.0   200\n",
            "4      Geacă  Îmbrăcăminte     500.0    25\n",
            "5    Hanorac  Îmbrăcăminte       NaN    40\n"
          ]
        }
      ],
      "source": [
        "# Exemplu 2: Salvarea și citirea unui fișier HDF5\n",
        "\n",
        "# Salvarea DataFrame-ului într-un fișier HDF5\n",
        "df_produse.to_hdf('magazin_date.h5', key='produse', format='table')\n",
        "\n",
        "print(\"DataFrame-ul a fost salvat în 'magazin_date.h5' sub cheia 'produse'.\")\n",
        "\n",
        "# Citirea datelor din fișierul HDF5\n",
        "df_din_hdf = pd.read_hdf('magazin_date.h5', 'produse')\n",
        "\n",
        "print(\"\\nDatele citite din HDF5:\")\n",
        "print(df_din_hdf)\n",
        "\n",
        "# OBS.: 'key' funcționează ca un nume pentru setul de date în interiorul\n",
        "# fișierului HDF5. Un singur fișier .h5 poate conține mai multe seturi de date,\n",
        "# fiecare cu propria sa cheie."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tItAwlfT6BZ0"
      },
      "outputs": [],
      "source": [
        "# __EXERCIȚIU__\n",
        "# Salvați DataFrame-ul `df_angajati` într-un fișier HDF5 numit 'companie.h5',\n",
        "# folosind cheia 'departament_it'.\n",
        "# Apoi, citiți datele înapoi într-un nou DataFrame și afișați-l pentru a\n",
        "# verifica.\n",
        "\n",
        "# HINT: Folosiți `df_angajati.to_hdf('companie.h5', key='departament_it')` și\n",
        "# apoi `pd.read_hdf('companie.h5', 'departament_it')`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVKtFi2d6BZ0"
      },
      "source": [
        "## Apache Arrow\n",
        "\n",
        "**Apache Arrow** nu este un format de fișier, ci o platformă de dezvoltare pentru lucrul cu date **în memorie**. Scopul său principal este de a standardiza formatul datelor tabulare în memoria RAM, astfel încât diferite sisteme (Pandas, Spark, baze de date) să poată schimba date între ele fără costisitoarele procese de serializare și deserializare (conversia datelor într-un format care poate fi stocat sau transmis și apoi înapoi).\n",
        "\n",
        "Pandas se integrează din ce în ce mai strâns cu PyArrow (implementarea Python pentru Arrow) pentru a oferi performanțe îmbunătățite și tipuri de date mai avansate, care pot gestiona mai bine valorile lipsă și pot accelera operațiunile.\n",
        "\n",
        "! Pentru a folosi Arrow, trebuie instalată biblioteca `pyarrow`: `pip install pyarrow`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6nEGq946BZ0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "423b46e2-5d3e-4970-ff4f-16263041fa84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame-ul a fost convertit într-un tabel Arrow.\n",
            "Schema tabelului Arrow:\n",
            "Produs: string\n",
            "Categorie: string\n",
            "Pret_RON: double\n",
            "Stoc: int64\n",
            "-- schema metadata --\n",
            "pandas: '{\"index_columns\": [{\"kind\": \"range\", \"name\": null, \"start\": 0, \"' + 711\n",
            "\n",
            "Tabelul Arrow a fost convertit înapoi în Pandas:\n",
            "      Produs     Categorie  Pret_RON  Stoc\n",
            "0     Tricou  Îmbrăcăminte      80.0   100\n",
            "1  Pantaloni  Îmbrăcăminte     250.0    50\n",
            "2    Adidași  Încălțăminte     400.0    75\n",
            "3      Șapcă     Accesorii      50.0   200\n",
            "4      Geacă  Îmbrăcăminte     500.0    25\n",
            "5    Hanorac  Îmbrăcăminte       NaN    40\n"
          ]
        }
      ],
      "source": [
        "# Exemplu: Conversia între Pandas DataFrame și Arrow Table\n",
        "import pyarrow as pa\n",
        "\n",
        "# 1. Convertim un DataFrame Pandas într-un tabel Arrow\n",
        "tabel_arrow = pa.Table.from_pandas(df_produse)\n",
        "\n",
        "print(\"DataFrame-ul a fost convertit într-un tabel Arrow.\")\n",
        "print(\"Schema tabelului Arrow:\")\n",
        "print(tabel_arrow.schema)\n",
        "\n",
        "# 2. Convertim tabelul Arrow înapoi într-un DataFrame Pandas\n",
        "df_inapoi_in_pandas = tabel_arrow.to_pandas()\n",
        "\n",
        "print(\"\\nTabelul Arrow a fost convertit înapoi în Pandas:\")\n",
        "print(df_inapoi_in_pandas)\n",
        "\n",
        "# OBS.: Conversia este foarte rapidă deoarece ambele biblioteci pot partaja același format de date în memorie.\n",
        "# Acest lucru este crucial pentru interoperabilitatea în ecosistemele de big data. [cite: 292]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXAiARM86BZ0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87c2da70-a57e-48e7-d502-bf5a43e549d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O serie Pandas cu tip de date susținut de PyArrow:\n",
            "0    -2.5\n",
            "1    10.1\n",
            "2    <NA>\n",
            "dtype: float[pyarrow]\n"
          ]
        }
      ],
      "source": [
        "# Exemplu: Crearea unei coloane cu tip de date PyArrow-backed\n",
        "\n",
        "serie_arrow = pd.Series([-2.5, 10.1, None], dtype=\"float32[pyarrow]\")\n",
        "\n",
        "print(\"O serie Pandas cu tip de date susținut de PyArrow:\")\n",
        "print(serie_arrow)\n",
        "\n",
        "# OBS.: Observați cum valoarea lipsă este reprezentată ca `<NA>`, specific\n",
        "# tipurilor de date Arrow, ceea ce oferă o gestionare mai consistentă a datelor\n",
        "# lipsă."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tmlesKgY6BZ1"
      },
      "outputs": [],
      "source": [
        "# __EXERCIȚIU__\n",
        "# 1. Convertiți DataFrame-ul `df_angajati` într-un tabel Apache Arrow.\n",
        "# 2. Afișați schema tabelului Arrow pentru a observa tipurile de date.\n",
        "# 3. Convertiți tabelul înapoi într-un DataFrame Pandas și afișați-l.\n",
        "\n",
        "# HINT: Folosiți `pa.Table.from_pandas()` și `tabel.to_pandas()`."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}