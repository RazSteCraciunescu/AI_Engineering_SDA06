{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V66TzsABTlOa"
      },
      "source": [
        "# Capitolul 1: Introducere în Machine Learning\n",
        "\n",
        "Bine ai venit în lumea fascinantă a **Inteligenței Artificiale**! În acest prim capitol, vom explora ce înseamnă **Machine Learning** (ML) sau *Învățarea Automată*. Gândește-te la ML ca la o metodă prin care calculatoarele învață din date, similar modului în care noi învățăm din experiență, fără a fi programate în mod explicit pentru fiecare sarcină. Importanța acestui concept este uriașă, deoarece permite automatizarea unor procese complexe, de la recunoașterea vocală până la diagnosticarea medicală."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxUkapoSTlOb"
      },
      "source": [
        "## De ce folosim Machine Learning?\n",
        "\n",
        "Să luăm un exemplu clasic: **filtrarea e-mailurilor SPAM**. În programarea tradițională, am scrie manual reguli pentru a detecta mesajele nedorite: \"dacă e-mailul conține cuvântul 'ofertă', marchează-l ca SPAM\". Dar ce se întâmplă când spammerii devin mai inventivi? Ar trebui să adăugăm constant noi reguli, un proces anevoios și ineficient.\n",
        "\n",
        "Aici intervine ML. În loc să scriem reguli, îi oferim algoritmului mii de exemple de e-mailuri, etichetate deja ca **SPAM** sau **non-SPAM** (acestea sunt datele de antrenament, *experience E*). Algoritmul \"învață\" singur ce cuvinte sau fraze sunt asociate cu mesajele SPAM și își creează propriile reguli. Astfel, modelul se poate adapta automat la noi tipuri de SPAM, fiind mult mai eficient și mai ușor de întreținut."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTCOzVw5TlOb"
      },
      "source": [
        "## Tipuri de Sisteme Machine Learning\n",
        "\n",
        "Există trei categorii principale de algoritmi ML:\n",
        "* **Învățare Supervizată (Supervised Learning)**: Este cel mai comun tip. Aici, datele de antrenament sunt etichetate, adică pentru fiecare exemplu avem atât datele de intrare (features), cât și rezultatul corect (label). Scopul este de a învăța o \"regulă\" care să mapeze intrările la ieșiri. Exemplul cu filtrul SPAM este un caz de învățare supervizată.\n",
        "* **Învățare Nesupevizată (Unsupervised Learning)**: În acest caz, datele nu sunt etichetate. Algoritmul încearcă să găsească singur structuri sau modele ascunse în date, cum ar fi gruparea clienților pe baza comportamentului de cumpărare (clustering).\n",
        "* **Învățare prin Recompensă (Reinforcement Learning)**: Aici, un \"agent\" învață să ia decizii într-un mediu pentru a maximiza o recompensă. Este folosit adesea în jocuri (ex: AlphaGo) sau robotică."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0FFbrypTlOc"
      },
      "source": [
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNWDKwTWTlOc"
      },
      "source": [
        "# Capitolul 2: Regresia - Prezicerea Valorilor Continue\n",
        "\n",
        "În acest capitol, ne vom concentra pe o sarcină fundamentală din **învățarea supervizată**: **regresia**. Problemele de regresie au ca scop prezicerea unei valori **numerice continue**. De exemplu, putem prezice prețul unei mașini pe baza vechimii și a kilometrajului, sau temperatura de mâine pe baza datelor meteo de astăzi.\n",
        "\n",
        "Spre deosebire de problemele de *clasificare* (unde rezultatul este o categorie, ex: SPAM/non-SPAM), în regresie, valoarea pe care o prezicem (numită **variabilă țintă** sau `label`) poate lua un număr infinit de valori într-un interval."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O10dNHinTlOc"
      },
      "source": [
        "## Regresia Liniară\n",
        "\n",
        "Cea mai simplă formă de regresie este **regresia liniară**. Să ne imaginăm că avem un set de date despre prețurile apartamentelor în funcție de suprafața lor. Dacă punem aceste date pe un grafic (scatterplot), unde pe axa X avem suprafața și pe axa Y avem prețul, vom observa probabil o tendință: pe măsură ce suprafața crește, și prețul crește.\n",
        "\n",
        "Regresia liniară încearcă să traseze **o linie dreaptă** care să treacă cât mai aproape de toate aceste puncte. Această linie reprezintă modelul nostru. Odată ce avem linia, putem folosi suprafața unui apartament nou (o valoare pe axa X) pentru a estima prețul său (valoarea corespunzătoare pe axa Y, citită de pe linie)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hY2ezp5iTlOc"
      },
      "source": [
        "## Funcția Ipoteză și Funcția de Cost\n",
        "\n",
        "Linia dreaptă din regresia liniară este descrisă de o ecuație matematică, numită **funcția ipoteză** (`hypothesis function`). Pentru o singură caracteristică (ex: suprafața), ecuația arată familiar:\n",
        "\n",
        "$$ h_\\theta(x) = \\theta_0 + \\theta_1 x $$\n",
        "\n",
        "* `x` este valoarea caracteristicii de intrare (suprafața).\n",
        "* `h_θ(x)` este valoarea prezisă (prețul estimat).\n",
        "* `θ₀` (theta zero) este **interceptul** – punctul unde linia taie axa Y (un fel de preț de bază).\n",
        "* `θ₁` (theta unu) este **panta** – cât de mult crește prețul pentru fiecare metru pătrat în plus.\n",
        "\n",
        "Dar cum știm care este cea mai bună linie? Putem trasa o infinitate de linii. Aici intervine **funcția de cost** (*cost function*), care măsoară cât de departe sunt predicțiile noastre de valorile reale. Scopul nostru este să găsim valorile pentru `θ₀` și `θ₁` care fac ca această eroare totală să fie **minimă**.\n",
        "\n",
        "Cea mai comună funcție de cost pentru regresie este **Eroarea Medie Pătratică** (**Mean Squared Error - MSE**). Pentru fiecare punct, calculăm diferența dintre prețul real și prețul prezis de linie, ridicăm această diferență la pătrat (pentru a avea doar valori pozitive și a penaliza erorile mari) și apoi facem media tuturor acestor erori pătratice. Linia \"perfectă\" este cea pentru care MSE este cel mai mic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KUQ5yC_ATlOc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae6cb2d8-c565-40e0-8695-f4257822288d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prețuri prezise: [110, 150, 210]\n",
            "Erori pătratice: [100, 0, 0]\n",
            "Eroarea Medie Pătratică (MSE): 33.333333333333336\n"
          ]
        }
      ],
      "source": [
        "# Exemplu 1: Calcul manual MSE\n",
        "# Să presupunem că avem 3 apartamente cu prețurile și suprafețele reale.\n",
        "suprafete_reale = [50, 70, 100] # m²\n",
        "preturi_reale = [100, 150, 210] # mii EUR\n",
        "\n",
        "# Modelul nostru (ipoteza) este: pret = 10 + 2 * suprafata\n",
        "# Adică θ₀ = 10, θ₁ = 2\n",
        "\n",
        "preturi_prezise = [(10 + 2 * s) for s in suprafete_reale]\n",
        "print(f\"Prețuri prezise: {preturi_prezise}\")\n",
        "\n",
        "# Calculăm erorile pătratice\n",
        "erori_patratice = [(real - prezis)**2 for real, prezis in zip(preturi_reale, preturi_prezise)]\n",
        "print(f\"Erori pătratice: {erori_patratice}\")\n",
        "\n",
        "# Calculăm MSE\n",
        "mse = sum(erori_patratice) / len(erori_patratice)\n",
        "print(f\"Eroarea Medie Pătratică (MSE): {mse}\")\n",
        "\n",
        "# OBS.: O valoare MSE mai mică indică un model mai bun. Dacă am alege alți θ₀\n",
        "# și θ₁, am obține un alt MSE."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "suprafete_reale = [50, 70, 100] # m²\n",
        "preturi_reale = [100, 150, 210] # mii EUR\n",
        "\n",
        "preturi_prezise = []\n",
        "\n",
        "for s in suprafete_reale:\n",
        "  preturi_prezise.append(s * 2 + 10)\n",
        "print(preturi_prezise)\n",
        "\n",
        "erori_patratice = []\n",
        "for i in range(len(preturi_reale)):\n",
        "  diferenta_preturi = preturi_reale[i] - preturi_prezise[i]\n",
        "  patratul_distantei = diferenta_preturi ** 2\n",
        "  erori_patratice.append(patratul_distantei)\n",
        "\n",
        "mse = sum(erori_patratice) / len(erori_patratice)\n",
        "print(mse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYGCiaZTs_gj",
        "outputId": "a95f2cab-6245-4ad1-fa0d-9e0291c4d2e6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[110, 150, 210]\n",
            "33.333333333333336\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "qYvZZak5TlOd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc3a76ef-6960-4172-a07c-de5084243999"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prețuri prezise: [110.0, 152.0, 215.0]\n",
            "Erori pătratice: [100.0, 4.0, 25.0]\n",
            "Eroarea Medie Pătratică (MSE): 43.0\n"
          ]
        }
      ],
      "source": [
        "# __EXERCIȚIU__\n",
        "# Se dă un nou model: pret = 5 + 2.1 * suprafata.\n",
        "# Calculează noul MSE folosind aceleași date reale ca în exemplul de mai sus.\n",
        "# Care model este mai bun? Cel din exemplu sau acesta?\n",
        "\n",
        "suprafete_reale = [50, 70, 100]\n",
        "preturi_reale = [100, 150, 210]\n",
        "\n",
        "preturi_prezise = [(5 + 2.1 * s) for s in suprafete_reale]\n",
        "print(f\"Prețuri prezise: {preturi_prezise}\")\n",
        "\n",
        "erori_patratice = [(real - prezis)**2 for real, prezis in zip(preturi_reale, preturi_prezise)]\n",
        "print(f\"Erori pătratice: {erori_patratice}\")\n",
        "\n",
        "mse = sum(erori_patratice) / len(erori_patratice)\n",
        "print(f\"Eroarea Medie Pătratică (MSE): {mse}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZ3gabtoTlOd"
      },
      "source": [
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rh0zvvjTTlOd"
      },
      "source": [
        "# Capitolul 3: Optimizarea Modelului - Cum Găsim Cea Mai Bună Linie?\n",
        "\n",
        "Am stabilit că scopul nostru este să minimizăm funcția de cost (MSE). Dar cum facem asta în mod eficient? Aici intervine un proces numit **optimizare**.\n",
        "\n",
        "Să ne imaginăm funcția de cost ca o vale. Axa X și Y ar reprezenta valorile posibile pentru `θ₀` și `θ₁`, iar axa Z (înălțimea) ar fi valoarea MSE. Noi vrem să găsim cel mai jos punct din această vale, numit **minim global** (*global minimum*). Acesta corespunde celor mai bune valori pentru parametrii noștri."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xb261Jx9TlOd"
      },
      "source": [
        "## Gradient Descent (Coborârea în Gradient)\n",
        "\n",
        "Cel mai popular algoritm pentru optimizare este **Gradient Descent**. Analogia este simplă: te afli pe un versant al văii și vrei să ajungi în cel mai jos punct. Ce faci? Te uiți în jur și faci un pas în direcția cea mai abruptă de coborâre. Repeți acest proces până când ajungi la fundul văii, unde panta este zero.\n",
        "\n",
        "Matematic, \"direcția cea mai abruptă de coborâre\" este dată de **gradient**, care este un fel de derivată pentru funcții cu mai multe variabile. Algoritmul funcționează astfel:\n",
        "1.  Începe cu valori aleatorii pentru `θ₀` și `θ₁` (te plasezi într-un punct aleatoriu pe vale).\n",
        "2.  Calculează gradientul funcției de cost în acel punct. Acesta ne spune în ce direcție să ne mișcăm.\n",
        "3.  Actualizează `θ₀` și `θ₁` făcând un mic pas în direcția opusă gradientului (la vale).\n",
        "4.  Repetă pașii 2 și 3 până când pașii devin foarte mici, adică am ajuns la un minim.\n",
        "\n",
        "### Rata de Învățare (Learning Rate - α)\n",
        "\n",
        "Cât de mare este \"pasul\" pe care îl facem la fiecare iterație? Acest lucru este controlat de un parametru numit **rata de învățare** (notat cu `α`).\n",
        "* Dacă **α este prea mic**, vom face pași foarte mici și va dura mult timp să ajungem la minim.\n",
        "* Dacă **α este prea mare**, riscăm să \"sărim\" peste minim și să nu ajungem niciodată la el (algoritmul poate diverge).\n",
        "\n",
        "Alegerea unei rate de învățare potrivite este crucială pentru succesul antrenării."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqR6awRHTlOd"
      },
      "outputs": [],
      "source": [
        "# Exemplu 2: O singură iterație de Gradient Descent (conceptual)\n",
        "# Să pornim de la modelul din Exercițiul 1: θ₀ = 5, θ₁ = 2.1\n",
        "theta_0_curent = 5\n",
        "theta_1_curent = 2.1\n",
        "rata_invatare = 0.0001\n",
        "\n",
        "# Să presupunem că am calculat gradientul și am obținut:\n",
        "# gradient_theta_0 = -20\n",
        "# gradient_theta_1 = -1500\n",
        "\n",
        "gradient_theta_0 = -20\n",
        "gradient_theta_1 = -1500\n",
        "\n",
        "# Actualizăm parametrii\n",
        "theta_0_nou = theta_0_curent - rata_invatare * gradient_theta_0\n",
        "theta_1_nou = theta_1_curent - rata_invatare * gradient_theta_1\n",
        "\n",
        "print(f\"Theta 0 nou: {theta_0_nou}\")\n",
        "print(f\"Theta 1 nou: {theta_1_nou}\")\n",
        "\n",
        "# OBS.: Noii parametri sunt mai apropiați de valorile optime. Am făcut un pas\n",
        "# mic \"la vale\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDy-eEUKTlOd"
      },
      "outputs": [],
      "source": [
        "# __EXERCIȚIU__\n",
        "# Presupunând că la următorul pas, gradientul este:\n",
        "# gradient_theta_0 = -18\n",
        "# gradient_theta_1 = -1350\n",
        "# Calculează noile valori pentru θ₀ și θ₁, pornind de la cele calculate în\n",
        "# exemplul de mai sus. Folosește aceeași rată de învățare.\n",
        "\n",
        "# HINT: Aplică formula de actualizare pe noile valori."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOo8DwY4TlOd"
      },
      "source": [
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XapH1T6WTlOd"
      },
      "source": [
        "# Capitolul 4: Regresia în Practică cu Scikit-Learn\n",
        "\n",
        "Din fericire, nu trebuie să implementăm Gradient Descent de la zero. Biblioteca **Scikit-Learn** (`sklearn`) este standardul în industrie pentru Machine Learning în Python și ne oferă unelte puternice pentru a crea și antrena modele rapid.\n",
        "\n",
        "Vom folosi clasa `LinearRegression` pentru a crea un model de regresie liniară. Procesul este simplu și urmează câțiva pași standard:\n",
        "1.  **Importă** clasa necesară (`LinearRegression`).\n",
        "2.  **Pregătește datele**: separă caracteristicile (features, de obicei notate cu `X`) de variabila țintă (label, notată cu `y`).\n",
        "3.  **Instanțiază modelul**: creează un obiect de tipul `LinearRegression`.\n",
        "4.  **Antrenează modelul**: folosește metoda `.fit(X, y)` pentru a porni procesul de optimizare (Scikit-Learn face Gradient Descent în spate pentru noi).\n",
        "5.  **Fă predicții**: folosește metoda `.predict()` pe date noi pentru a obține estimări."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "TG3grvUxTlOd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a22b7ca1-89d1-4ec0-eccc-30b52ab0da53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intercept (θ₀): 0.51\n",
            "Panta (θ₁): 2.08\n",
            "\n",
            "Prețul prezis pentru 85 m² este: 177.01 mii EUR\n"
          ]
        }
      ],
      "source": [
        "# Exemplu 3: Regresie Liniară cu Scikit-Learn\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# 1. Pregătirea datelor\n",
        "# X trebuie să fie un array 2D (chiar și pentru o singură caracteristică)\n",
        "suprafete = np.array([50, 70, 100, 120, 150]).reshape(-1, 1) # Transformăm acest vector într-un vector-coloană (2D)\n",
        "preturi = np.array([100, 150, 210, 250, 310])\n",
        "\n",
        "# 2. Instanțierea modelului\n",
        "model_regresie = LinearRegression()\n",
        "\n",
        "# 3. Antrenarea modelului\n",
        "model_regresie.fit(suprafete, preturi)\n",
        "\n",
        "# 4. Vizualizarea parametrilor învățați (θ₀ și θ₁)\n",
        "theta_0 = model_regresie.intercept_\n",
        "theta_1 = model_regresie.coef_[0]\n",
        "print(f\"Intercept (θ₀): {theta_0:.2f}\")\n",
        "print(f\"Panta (θ₁): {theta_1:.2f}\")\n",
        "\n",
        "# 5. Fă o predicție pentru un apartament de 85 m²\n",
        "suprafata_noua = np.array([[85]])\n",
        "pret_prezis = model_regresie.predict(suprafata_noua)\n",
        "print(f\"\\nPrețul prezis pentru 85 m² este: {pret_prezis[0]:.2f} mii EUR\")\n",
        "\n",
        "# OBS.: Metoda .fit() a găsit pentru noi cele mai bune valori pentru θ₀ și θ₁\n",
        "# care minimizează MSE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JwC5zGTeTlOe"
      },
      "outputs": [],
      "source": [
        "# __EXERCIȚIU__\n",
        "# Folosind modelul antrenat mai sus, prezice prețurile pentru următoarele\n",
        "# suprafețe: 60, 90 și 130 m². Afișează rezultatele.\n",
        "\n",
        "# HINT: Poți trimite o listă de valori către metoda .predict(). Asigură-te că\n",
        "# este un array 2D."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOb-MU5_TlOe"
      },
      "source": [
        "## Dincolo de Liniar: Alte Modele de Regresie\n",
        "\n",
        "Relația dintre variabile nu este întotdeauna liniară. Uneori, prețul poate crește mai repede la suprafețe mai mari, descriind o curbă. Pentru astfel de cazuri, regresia liniară nu este suficientă.\n",
        "\n",
        "Scikit-Learn oferă o varietate de alte modele de regresie, capabile să surprindă relații mai complexe:\n",
        "* **Regresia Polinomială**: Potrivește o curbă (un polinom) în loc de o linie dreaptă.\n",
        "* **Arbori de Decizie (Decision Tree Regressor)**: Împarte datele în regiuni și prezice o valoare constantă pentru fiecare regiune. Sunt foarte buni la a captura relații non-liniare complexe.\n",
        "* **Support Vector Regression (SVR)**: O variantă a Support Vector Machines pentru probleme de regresie.\n",
        "\n",
        "Alegerea modelului potrivit depinde de specificul datelor și al problemei. În fișierul de practică, vei avea ocazia să compari performanța unui model `LinearRegression` cu cea a unui `DecisionTreeRegressor`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "017ccc71"
      },
      "source": [
        "## Decision Tree Regressor (Arbore de Decizie pentru Regresie)\n",
        "\n",
        "**Decision Tree Regressor** este un algoritm versatil care, spre deosebire de regresia liniară, poate modela relații **non-liniare** complexe în date. Funcționează prin împărțirea spațiului de intrare în regiuni simple și prezicerea unei valori constante (de obicei media valorilor din datele de antrenament din acea regiune) pentru orice punct din acea regiune.\n",
        "\n",
        "Imaginează-ți că vrei să prezici prețul unei mașini. Un arbore de decizie ar putea pune întrebări succesive:\n",
        "1. Este mașina mai veche de 5 ani?\n",
        "    * Dacă da, mergi la stânga.\n",
        "    * Dacă nu, mergi la dreapta.\n",
        "2. (Dacă ai mers la stânga) Are mașina peste 100.000 km?\n",
        "    * Dacă da, mergi la stânga jos.\n",
        "    * Dacă nu, mergi la dreapta jos.\n",
        "3. (Dacă ai mers la dreapta) Este mașina marca X?\n",
        "    * ... și așa mai departe.\n",
        "\n",
        "Fiecare \"întrebare\" este o **decizie** bazată pe o caracteristică (ex: vârsta, kilometrajul, marca). Arborele construiește o serie de astfel de decizii, formând ramuri, până ajunge la un **nod frunză** (*leaf node*). Fiecare nod frunză corespunde unei **regiuni** din spațiul de intrare, iar predicția pentru orice punct care ajunge la acea frunză este o valoare specifică acelei frunze (calculată în timpul antrenării, de obicei ca media valorilor țintă din datele de antrenament care au ajuns în acea frunză).\n",
        "\n",
        "**Cum se construiește arborele?**\n",
        "\n",
        "Procesul este iterativ și se bazează pe minimizarea unei măsuri a \"impurității\" sau a erorii în fiecare nod. Pentru regresie, măsura folosită este de obicei **MSE** (Eroarea Medie Pătratică) sau **MAE** (Eroarea Absolută Medie). La fiecare pas, algoritmul caută cea mai bună caracteristică și cel mai bun prag de împărțire care reduce cel mai mult eroarea în cele două sub-noduri rezultate. Acest proces se repetă recursiv până când se atinge o condiție de oprire (ex: adâncimea maximă a arborelui, numărul minim de exemple într-un nod frunză).\n",
        "\n",
        "**Avantaje:**\n",
        "* Poate modela relații non-liniare.\n",
        "* Ușor de înțeles și interpretat (dacă arborele nu este prea adânc).\n",
        "* Nu necesită scalarea caracteristicilor.\n",
        "\n",
        "**Dezavantaje:**\n",
        "* Poate fi predispus la **supra-antrenare** (*overfitting*), adică se potrivește prea bine cu datele de antrenament, dar generalizează slab pe date noi. Acest lucru se întâmplă dacă arborele devine prea adânc și învață zgomotul din date.\n",
        "* Este instabil: mici modificări în datele de antrenament pot duce la un arbore complet diferit.\n",
        "\n",
        "Datorită riscului de supra-antrenare, Decision Tree Regressors sunt adesea folosiți ca blocuri de construcție pentru algoritmi mai avansați, cum ar fi **Random Forests** sau **Gradient Boosting**, care combină mai mulți arbori pentru a obține o performanță mai robustă."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELTpSdhMTlOe"
      },
      "outputs": [],
      "source": [
        "# Exemplu 4: Antrenarea unui DecisionTreeRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "# Folosim aceleași date ca înainte\n",
        "suprafete = np.array([50, 70, 100, 120, 150]).reshape(-1, 1)\n",
        "preturi = np.array([100, 150, 210, 250, 310])\n",
        "\n",
        "# Instanțiere și antrenare\n",
        "model_arbore = DecisionTreeRegressor()\n",
        "model_arbore.fit(suprafete, preturi)\n",
        "\n",
        "# Predicție pentru 85 m²\n",
        "pret_prezis_arbore = model_arbore.predict(suprafata_noua)\n",
        "print(f\"Prețul prezis de arborele de decizie pentru 85 m² este: {pret_prezis_arbore[0]:.2f} mii EUR\")\n",
        "\n",
        "# OBS.: Rezultatul poate fi diferit de cel al regresiei liniare, deoarece\n",
        "# arborele învață relațiile într-un mod fundamental diferit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QDxTSV2sTlOe"
      },
      "outputs": [],
      "source": [
        "# __EXERCIȚIU__\n",
        "# Folosind modelul antrenat mai sus, încearcă acum să prezici prețurile pentru\n",
        "# aceleași suprafețe ca la exercițiul anterior: 60, 90 și 130 m².\n",
        "\n",
        "# Compară rezultatele celor două modele. Care pare mai bun?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}