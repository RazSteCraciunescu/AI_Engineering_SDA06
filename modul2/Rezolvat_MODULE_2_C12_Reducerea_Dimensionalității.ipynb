{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5Mv-ijkDyN7"
      },
      "source": [
        "# Capitolul 1: Introducere în Reducerea Dimensionalității"
      ],
      "id": "s5Mv-ijkDyN7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYxlNSbIDyN9"
      },
      "source": [
        "Bun venit la cursul despre **reducerea dimensionalității**! În machine learning, când lucrăm cu seturi de date, fiecare coloană (cu excepția țintei) este considerată o **dimensiune** sau o **caracteristică** (*feature*). Un set de date cu 10 caracteristici este un set de date 10-dimensional. Deși poate părea că mai multe date (dimensiuni) înseamnă informații mai bune, acest lucru nu este întotdeauna adevărat. Uneori, un număr foarte mare de dimensiuni poate crea probleme, un fenomen cunoscut sub numele de **\"Blestemul Dimensionalității\"** (*Curse of Dimensionality*)."
      ],
      "id": "xYxlNSbIDyN9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUAp-CbFDyN9"
      },
      "source": [
        "## Blestemul Dimensionalității (Curse of Dimensionality)"
      ],
      "id": "VUAp-CbFDyN9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkpKsg0NDyN-"
      },
      "source": [
        "Acest concept descrie provocările care apar atunci când lucrăm cu date în spații cu multe dimensiuni. Imaginați-vă că aveți puncte pe o linie (1D). Acestea sunt destul de apropiate. Acum, adăugați aceleași puncte pe o foaie de hârtie (2D) și apoi într-o cutie (3D). Pe măsură ce adăugăm dimensiuni, distanța medie dintre puncte crește, iar datele devin din ce în ce mai **rare** (*sparse*).\n",
        "\n",
        "Acest lucru duce la câteva probleme:\n",
        "* **Complexitate computațională crescută**: Mai multe dimensiuni înseamnă mai multă putere de calcul necesară pentru a antrena modele.\n",
        "* **Diminuarea utilității distanței**: Măsurătorile de distanță (cum ar fi distanța euclidiană) devin mai puțin relevante, afectând algoritmi precum K-Nearest Neighbors.\n",
        "* **Supra-antrenare (Overfitting)**: Modelele pot învăța zgomotul din date în loc de tiparele reale, deoarece există prea multe caracteristici din care să \"învețe\"."
      ],
      "id": "wkpKsg0NDyN-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jnt2SFIoDyN-"
      },
      "outputs": [],
      "source": [
        "# Exemplu 1: Vizualizarea rarității datelor\n",
        "# Să generăm 20 de puncte aleatorii în spații 1D, 2D și 3D pentru a observa cum se distanțează.\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generăm datele\n",
        "np.random.seed(42)\n",
        "points_1d = np.random.rand(20, 1)\n",
        "points_2d = np.random.rand(20, 2)\n",
        "points_3d = np.random.rand(20, 3)\n",
        "\n",
        "# Creăm figura pentru grafice\n",
        "fig = plt.figure(figsize=(15, 5))\n",
        "\n",
        "# Grafic 1D\n",
        "ax1 = fig.add_subplot(1, 3, 1)\n",
        "ax1.scatter(points_1d, np.zeros_like(points_1d), c='blue')\n",
        "ax1.set_title('20 Puncte în 1D')\n",
        "ax1.set_xlabel('Dimensiunea 1')\n",
        "ax1.get_yaxis().set_visible(False)\n",
        "\n",
        "# Grafic 2D\n",
        "ax2 = fig.add_subplot(1, 3, 2)\n",
        "ax2.scatter(points_2d[:, 0], points_2d[:, 1], c='red')\n",
        "ax2.set_title('20 Puncte în 2D')\n",
        "ax2.set_xlabel('Dimensiunea 1')\n",
        "ax2.set_ylabel('Dimensiunea 2')\n",
        "\n",
        "# Grafic 3D\n",
        "ax3 = fig.add_subplot(1, 3, 3, projection='3d')\n",
        "ax3.scatter(points_3d[:, 0], points_3d[:, 1], points_3d[:, 2], c='green')\n",
        "ax3.set_title('20 Puncte în 3D')\n",
        "ax3.set_xlabel('Dimensiunea 1')\n",
        "ax3.set_ylabel('Dimensiunea 2')\n",
        "ax3.set_zlabel('Dimensiunea 3')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# OBS.: Observați cum în graficul 3D, punctele par mult mai împrăștiate și\n",
        "# izolate decât pe linia 1D. Aceasta este o demonstrație vizuală a modului în\n",
        "# care datele devin mai rare pe măsură ce dimensiunile cresc."
      ],
      "id": "Jnt2SFIoDyN-"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGb_Ud2DDyN_"
      },
      "source": [
        "## De ce reducem dimensionalitatea?\n",
        "\n",
        "**Reducerea dimensionalității** este procesul de a micșora numărul de caracteristici (dimensiuni) dintr-un set de date, păstrând în același timp cât mai multă informație relevantă. Facem acest lucru pentru a:\n",
        "\n",
        "* **Îmbunătăți eficiența computațională**: Modelele se antrenează mai rapid pe date cu mai puține coloane.\n",
        "* **Utiliza mai puțin spațiu de memorie**: Seturile de date devin mai mici și mai ușor de gestionat.\n",
        "* **Elimina zgomotul și redundanța**: Unele caracteristici pot fi irelevante (zgomot) sau pot oferi aceeași informație ca altele (redundante).\n",
        "* **Evita supra-antrenarea (overfitting)**: Un model mai simplu (cu mai puține caracteristici) este mai puțin predispus la overfitting.\n",
        "* **Îmbunătăți vizualizările**: Este imposibil să vizualizăm date în mai mult de 3 dimensiuni. Reducerea la 2D sau 3D ne ajută să înțelegem structura datelor."
      ],
      "id": "pGb_Ud2DDyN_"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7t0cXFfXDyN_"
      },
      "source": [
        "## Metode de Reducere a Dimensionalității"
      ],
      "id": "7t0cXFfXDyN_"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DhTbTMIDyOA"
      },
      "source": [
        "Există două abordări principale pentru a reduce dimensionalitatea:\n",
        "\n",
        "1.  **Selecția de Caracteristici (Feature Selection)**: Alegem un subset al celor mai importante caracteristici din setul de date original și renunțăm la celelalte.\n",
        "2.  **Extragerea de Caracteristici (Feature Extraction)**: Creăm caracteristici noi, care sunt combinații ale celor vechi, și folosim aceste caracteristici noi pentru a reprezenta datele."
      ],
      "id": "0DhTbTMIDyOA"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZrUu3WODyOA"
      },
      "source": [
        "### 1. Selecția de Caracteristici (Feature Selection)"
      ],
      "id": "KZrUu3WODyOA"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9uWEPSMDyOA"
      },
      "source": [
        "Această metodă este ca și cum am face \"curățenie\" în setul de date. Ne uităm la toate caracteristicile disponibile și le păstrăm doar pe cele care sunt cu adevărat critice pentru a rezolva problema.\n",
        "\n",
        "De exemplu, dacă vrem să prezicem prețul unei case, caracteristici precum `suprafața utilă`, `numărul de camere` și `locația` sunt esențiale. Caracteristici precum `culoarea ușii de la intrare` sau `numele fostului proprietar` sunt, cel mai probabil, irelevante și pot fi eliminate."
      ],
      "id": "H9uWEPSMDyOA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvJrYrOrDyOB"
      },
      "outputs": [],
      "source": [
        "# __EXERCIȚIU__\n",
        "# Să presupunem că avem următorul DataFrame pandas cu date despre studenți.\n",
        "# Obiectivul nostru este să prezicem nota finală (`nota_finala`).\n",
        "# Inspectați vizual coloanele și creați un nou DataFrame, `studenti_modificat`,\n",
        "# care conține doar coloanele pe care le considerați relevante pentru a prezice\n",
        "# nota. Justificați-vă alegerea într-un comentariu.\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "data_studenti = {\n",
        "    'id_student': [101, 102, 103, 104, 105],\n",
        "    'nume_student': ['Popescu Ion', 'Ionescu Ana', 'Georgescu V.', 'Marinescu G.', 'Radulescu M.'],\n",
        "    'ore_studiu_saptamanal': [15, 20, 8, 12, 18],\n",
        "    'note_tema': [8, 9, 6, 7, 9],\n",
        "    'prezenta_curs': [0.95, 0.98, 0.70, 0.85, 0.99],\n",
        "    'culoare_ghiozdan': ['albastru', 'rosu', 'negru', 'albastru', 'verde'],\n",
        "    'nota_finala': [9, 10, 7, 8, 10]\n",
        "}\n",
        "\n",
        "studenti_df = pd.DataFrame(data_studenti)\n",
        "\n",
        "# Justificare: ... (completați aici)\n",
        "coloane_relevante = [] # Completați lista cu numele coloanelor relevante\n",
        "\n",
        "studenti_modificat = studenti_df[coloane_relevante]\n",
        "\n",
        "print(\"DataFrame original:\")\n",
        "print(studenti_df)\n",
        "print(\"\\nDataFrame după selecția de caracteristici:\")\n",
        "print(studenti_modificat)\n",
        "\n",
        "\n",
        "# HINT: Gândiți-vă ce factori influențează performanța unui student."
      ],
      "id": "pvJrYrOrDyOB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqf3LN4DDyOB"
      },
      "source": [
        "### 2. Extragerea de Caracteristici (Feature Extraction)\n",
        "\n",
        "Spre deosebire de selecție, unde doar alegem din ce există, extragerea de caracteristici **transformă** datele. Această metodă generează un set complet nou de caracteristici, prin combinarea celor vechi. Noile caracteristici sunt de obicei mai puține la număr, dar condensează informația importantă din setul de date original.\n",
        "\n",
        "Imaginați-vă că priviți un obiect 3D. Îl puteți proiecta pe o foaie 2D (umbra sa). Ați redus dimensionalitatea de la 3 la 2, dar umbra încă păstrează multe informații despre forma obiectului. Algoritmii de extragere fac ceva similar: găsesc cele mai bune \"proiecții\" ale datelor într-un spațiu cu mai puține dimensiuni.\n",
        "\n",
        "Cei mai populari algoritmi pentru extragerea de caracteristici sunt **Principal Component Analysis (PCA)** și **t-distributed Stochastic Neighbour Embedding (t-SNE)**."
      ],
      "id": "cqf3LN4DDyOB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIebFmJaDyOB"
      },
      "source": [
        "## Principal Component Analysis (PCA)\n",
        "\n",
        "**PCA** este cel mai popular algoritm de reducere a dimensionalității. Este o tehnică **liniară** care transformă setul de date într-un nou sistem de coordonate. În acest nou sistem, caracteristicile (numite **componente principale**) sunt **necorelate** liniar, iar cea mai mare parte a **varianței** (informației) din date este concentrată în primele câteva componente.\n",
        "\n",
        "Gândiți-vă la un nor de puncte în 3D care are forma unei elipse alungite. PCA va găsi o nouă axă (prima componentă principală) care trece de-a lungul celei mai mari lungimi a elipsei, deoarece aceasta este direcția cu cea mai mare variație a datelor. A doua componentă va fi perpendiculară pe prima și va descrie următoarea cea mai mare variație, și așa mai departe."
      ],
      "id": "jIebFmJaDyOB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qx7Mcc2rDyOB"
      },
      "outputs": [],
      "source": [
        "# Pentru a lucra cu exemplele, vom clona un repository cu seturi de date\n",
        "!git clone https://github.com/matzim95/ML-datasets"
      ],
      "id": "qx7Mcc2rDyOB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uY8NUxEyDyOB"
      },
      "outputs": [],
      "source": [
        "# Exemplu 1: Încărcarea datelor și vizualizarea corelației\n",
        "# Vom folosi setul de date 'wine', care are 13 caracteristici despre diferite\n",
        "# vinuri.\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df_wine = pd.read_csv('ML-datasets/wine.csv')\n",
        "\n",
        "# Separăm caracteristicile de țintă\n",
        "X_wine = df_wine.drop('Class', axis=1)\n",
        "y_wine = df_wine['Class']\n",
        "\n",
        "# Să ne uităm la corelația dintre 'Total phenols' și 'Flavanoids'\n",
        "print(f\"Corelația inițială: {X_wine['Total phenols'].corr(X_wine['Flavanoids']):.4f}\")\n",
        "\n",
        "sns.scatterplot(data=X_wine, x='Total phenols', y='Flavanoids')\n",
        "plt.title('Corelație puternică între caracteristicile originale')\n",
        "plt.show()\n",
        "\n",
        "# OBS.: Există o corelație liniară puternică între aceste două caracteristici.\n",
        "# PCA este eficient în a reduce această redundanță."
      ],
      "id": "uY8NUxEyDyOB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VI_DcA0wDyOB"
      },
      "outputs": [],
      "source": [
        "# Exemplu 2: Aplicarea PCA și verificarea corelației componentelor\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# IMPORTANT: PCA este sensibil la scala datelor. Este esențial să\n",
        "# standardizăm datele înainte de a aplica PCA.\n",
        "scaler = StandardScaler()\n",
        "X_wine_scaled = scaler.fit_transform(X_wine)\n",
        "\n",
        "# Inițializăm PCA. Fără a specifica n_components, va crea atâtea componente\n",
        "# câte caracteristici originale sunt.\n",
        "pca = PCA()\n",
        "X_wine_pca = pca.fit_transform(X_wine_scaled)\n",
        "\n",
        "# Transformăm rezultatul înapoi într-un DataFrame pentru a fi mai lizibil\n",
        "df_wine_pca = pd.DataFrame(X_wine_pca, columns=[f'PC{i+1}' for i in range(X_wine.shape[1])])\n",
        "\n",
        "# Verificăm corelația dintre primele două componente principale (PC1 și PC2)\n",
        "print(f\"Corelația dintre PC1 și PC2: {df_wine_pca['PC1'].corr(df_wine_pca['PC2']):.10f}\")\n",
        "\n",
        "sns.scatterplot(data=df_wine_pca, x='PC1', y='PC2')\n",
        "plt.title('Componentele principale sunt necorelate')\n",
        "plt.show()\n",
        "\n",
        "# OBS.: Corelația dintre primele două componente principale este practic zero.\n",
        "# PCA a reușit să creeze noi caracteristici care sunt independente liniar."
      ],
      "id": "VI_DcA0wDyOB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MajX2Wa2DyOC"
      },
      "outputs": [],
      "source": [
        "# __EXERCIȚIU__\n",
        "# Încarcă setul de date 'iris' din 'ML-datasets/iris.csv'.\n",
        "# 1. Alege două coloane care crezi că sunt corelate și calculează/afișează\n",
        "# corelația dintre ele.\n",
        "# 2. Standardizează datele (fără coloana 'species').\n",
        "# 3. Aplică PCA pe datele standardizate.\n",
        "# 4. Calculează și afișează corelația dintre primele două componente principale\n",
        "# (PC1 și PC2).\n",
        "\n",
        "# Încarcă datele\n",
        "df_iris = pd.read_csv('ML-datasets/iris.csv')\n",
        "X_iris = df_iris.drop('species', axis=1)\n",
        "\n",
        "# 1. Calculează corelația inițială\n",
        "corelatie_initiala = # ...\n",
        "print(f\"Corelația inițială aleasă: {corelatie_initiala:.4f}\")\n",
        "\n",
        "# 2. Standardizează datele\n",
        "scaler_iris = StandardScaler()\n",
        "X_iris_scaled = # ...\n",
        "\n",
        "# 3. Aplică PCA\n",
        "pca_iris = PCA()\n",
        "X_iris_pca = # ...\n",
        "df_iris_pca = pd.DataFrame(X_iris_pca, columns=['PC1', 'PC2', 'PC3', 'PC4'])\n",
        "\n",
        "# 4. Calculează corelația finală\n",
        "corelatie_pca = # ...\n",
        "print(f\"Corelația dintre PC1 și PC2 după PCA: {corelatie_pca:.10f}\")\n",
        "\n",
        "# HINT: Urmează pașii din exemplul anterior. Coloanele 'petal length (cm)' și\n",
        "# 'petal width (cm)' sunt bune candidate pentru corelație."
      ],
      "id": "MajX2Wa2DyOC"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYnMtNNoDyOC"
      },
      "source": [
        "### Varianța Explicată (Explained Variance)"
      ],
      "id": "LYnMtNNoDyOC"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2I0hKz8zDyOC"
      },
      "source": [
        "După aplicarea PCA, nu toate componentele principale sunt la fel de \"importante\". Primele componente captează cea mai mare parte a variabilității (informației) din date. Putem analiza atributul `explained_variance_ratio_` al modelului PCA pentru a vedea ce procent din varianța totală este explicat de fiecare componentă.\n",
        "\n",
        "Acest lucru ne ajută să decidem câte componente să păstrăm. Dacă primele 2-3 componente explică, de exemplu, 95% din varianța totală, putem renunța la celelalte fără a pierde prea multă informație."
      ],
      "id": "2I0hKz8zDyOC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lfrq2noODyOC"
      },
      "outputs": [],
      "source": [
        "# Exemplu 3: Analiza varianței explicate\n",
        "\n",
        "# Folosim modelul pca antrenat pe datele 'wine'\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "\n",
        "print(\"Varianța explicată de fiecare componentă:\")\n",
        "for i, variance in enumerate(explained_variance):\n",
        "    print(f\"  PC{i+1}: {variance*100:.2f}%\")\n",
        "\n",
        "# Afișăm varianța cumulativă\n",
        "cumulative_variance = np.cumsum(explained_variance)\n",
        "print(\"\\nVarianța cumulativă:\")\n",
        "for i, variance in enumerate(cumulative_variance):\n",
        "    print(f\"  Până la PC{i+1}: {variance*100:.2f}%\")\n",
        "\n",
        "# Vizualizăm varianța explicată cu un grafic\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(range(1, len(explained_variance) + 1), explained_variance, alpha=0.7, align='center', label='Varianța individuală')\n",
        "plt.step(range(1, len(cumulative_variance) + 1), cumulative_variance, where='mid', label='Varianța cumulativă')\n",
        "plt.ylabel('Procent de varianță explicată')\n",
        "plt.xlabel('Componenta principală')\n",
        "plt.title('Varianța Explicată de Componentele Principale (Wine)')\n",
        "plt.legend(loc='best')\n",
        "plt.show()\n",
        "\n",
        "# OBS.: Primele două componente (PC1 și PC2) explică împreună aproximativ 55%\n",
        "# din varianța totală. Primele 8 componente explică peste 90%. Acest grafic\n",
        "# este esențial pentru a decide câte dimensiuni să păstrăm."
      ],
      "id": "lfrq2noODyOC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cx7i91yJDyOC"
      },
      "outputs": [],
      "source": [
        "# __EXERCIȚIU__\n",
        "# Folosind modelul PCA antrenat anterior pe setul de date 'iris' (`pca_iris`),\n",
        "# 1. Calculează și afișează varianța explicată individuală și cumulativă pentru\n",
        "# fiecare componentă.\n",
        "# 2. Creează un grafic similar cu cel din exemplu pentru a vizualiza varianța\n",
        "# explicată.\n",
        "# 3. Răspunde într-un comentariu: Câte componente principale explică peste 95%\n",
        "# din varianța totală?\n",
        "\n",
        "# 1. Calculează varianța\n",
        "variance_iris = # ...\n",
        "cumulative_variance_iris = # ...\n",
        "\n",
        "print(\"Varianța explicată (Iris):\") # ...\n",
        "\n",
        "print(\"\\nVarianța cumulativă (Iris):\") # ...\n",
        "\n",
        "# 2. Creează graficul\n",
        "# ...\n",
        "\n",
        "# 3. Răspuns:\n",
        "# ...\n",
        "\n",
        "# HINT: Atributul este `explained_variance_ratio_`. Folosește `np.cumsum()`\n",
        "# pentru suma cumulativă."
      ],
      "id": "Cx7i91yJDyOC"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43T0ju2MDyOC"
      },
      "source": [
        "### Reducerea Efectivă a Dimensionalității"
      ],
      "id": "43T0ju2MDyOC"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FG4HkgqVDyOC"
      },
      "source": [
        "Acum că știm cum să analizăm varianța, putem folosi PCA pentru a reduce efectiv numărul de dimensiuni. Facem acest lucru specificând parametrul `n_components` la inițializarea modelului PCA. De exemplu, `PCA(n_components=2)` va reduce setul de date la doar 2 dimensiuni."
      ],
      "id": "FG4HkgqVDyOC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PRgohiZ3DyOC"
      },
      "outputs": [],
      "source": [
        "# Exemplu 4: Reducerea setului de date 'wine' la 2 dimensiuni\n",
        "\n",
        "# Refolosim datele standardizate X_wine_scaled\n",
        "\n",
        "# Inițializăm PCA pentru a păstra doar 2 componente\n",
        "pca_2d = PCA(n_components=2)\n",
        "\n",
        "# Aplicăm transformarea\n",
        "X_wine_reduced = pca_2d.fit_transform(X_wine_scaled)\n",
        "\n",
        "print(f\"Dimensiunea originală a datelor: {X_wine_scaled.shape}\")\n",
        "print(f\"Dimensiunea redusă a datelor: {X_wine_reduced.shape}\")\n",
        "\n",
        "# Vizualizăm datele reduse, colorând punctele în funcție de clasa de vin\n",
        "# (ținta y_wine)\n",
        "plt.figure(figsize=(10, 7))\n",
        "wine_labels_numeric = y_wine.astype('category').cat.codes\n",
        "scatter = plt.scatter(X_wine_reduced[:, 0], X_wine_reduced[:, 1], c=wine_labels_numeric, cmap='viridis', edgecolor='k', alpha=0.8)\n",
        "plt.title('Setul de date Wine redus la 2D cu PCA')\n",
        "plt.xlabel('Prima Componentă Principală (PC1)')\n",
        "plt.ylabel('A Doua Componentă Principală (PC2)')\n",
        "plt.legend(handles=scatter.legend_elements()[0], labels=list(df_wine['Class'].unique()))\n",
        "plt.show()\n",
        "\n",
        "# OBS.: Chiar și după reducerea de la 13 la 2 dimensiuni, putem observa o\n",
        "# separare destul de bună între cele trei clase de vin.\n",
        "# Acest lucru demonstrează că primele două componente au păstrat multă\n",
        "# informație relevantă pentru clasificare."
      ],
      "id": "PRgohiZ3DyOC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXKK3trZDyOD"
      },
      "outputs": [],
      "source": [
        "# __EXERCIȚIU__\n",
        "# 1. Redu setul de date standardizat 'iris' (`X_iris_scaled`) la 2 dimensiuni\n",
        "# folosind PCA.\n",
        "# 2. Creează un scatter plot al datelor reduse.\n",
        "# 3. Colorează punctele în funcție de coloana 'species' din DataFrame-ul\n",
        "# original `df_iris`.\n",
        "\n",
        "# 1. Redu dimensionalitatea\n",
        "pca_iris_2d = PCA(n_components=2)\n",
        "X_iris_reduced = # ...\n",
        "\n",
        "# Pregătește etichetele pentru colorare\n",
        "iris_labels = df_iris['species'].astype('category').cat.codes\n",
        "\n",
        "# 2 & 3. Creează graficul\n",
        "plt.figure(figsize=(10, 7))\n",
        "# ...\n",
        "plt.title('Setul de date Iris redus la 2D cu PCA')\n",
        "plt.xlabel('PC1')\n",
        "plt.ylabel('PC2')\n",
        "plt.show()\n",
        "\n",
        "# HINT: Parametrul `c` din `plt.scatter` poate accepta o serie pandas cu\n",
        "# valorile țintă (speciile), dar va fi necesar să transformați coloana\n",
        "# 'species' în numere (ex: folosind .astype('category').cat.codes)."
      ],
      "id": "sXKK3trZDyOD"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yK-aOpnADyOD"
      },
      "source": [
        "## t-distributed Stochastic Neighbour Embedding (t-SNE)"
      ],
      "id": "yK-aOpnADyOD"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xavnXCOxDyOD"
      },
      "source": [
        "**t-SNE** este o altă tehnică populară de extragere a caracteristicilor, dar, spre deosebire de PCA, este **non-liniară**. Este utilizată în principal pentru **vizualizarea** datelor cu multe dimensiuni.\n",
        "\n",
        "Scopul principal al t-SNE nu este să maximizeze varianța, ci să păstreze **structura locală** a datelor. Adică, punctele care sunt apropiate unele de altele în spațiul cu multe dimensiuni vor rămâne apropiate și în spațiul redus (de obicei 2D sau 3D). Acest lucru face t-SNE extrem de bun la identificarea și vizualizarea clusterelor din date.\n",
        "\n",
        "**Diferențe cheie față de PCA:**\n",
        "* **Complexitate**: t-SNE este mult mai costisitor din punct de vedere computațional decât PCA.\n",
        "* **Scop**: PCA este o tehnică generală de preprocesare. t-SNE este folosit aproape exclusiv pentru vizualizare.\n",
        "* **Interpretare**: Axele rezultate în urma t-SNE nu au o interpretare directă (spre deosebire de componentele principale din PCA, care reprezintă direcții de varianță)."
      ],
      "id": "xavnXCOxDyOD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G6KHxxbDDyOD"
      },
      "outputs": [],
      "source": [
        "df_iris = pd.read_csv('ML-datasets/iris.csv')\n",
        "X_iris = df_iris.drop('species', axis=1)\n",
        "iris_labels = df_iris['species'].astype('category').cat.codes\n",
        "\n",
        "# Exemplu 5: Aplicarea t-SNE pe setul de date 'iris'\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Folosim datele originale X_iris, deoarece t-SNE nu este la fel de sensibil la\n",
        "# scală ca PCA, deși standardizarea este în general o practică bună.\n",
        "\n",
        "# Inițializăm t-SNE pentru a reduce datele la 2 componente.\n",
        "# `learning_rate` și `perplexity` sunt hiperparametri importanți.\n",
        "# `random_state` asigură reproductibilitatea.\n",
        "tsne = TSNE(n_components=2, learning_rate='auto', init='random', random_state=42)\n",
        "\n",
        "# Aplicăm transformarea\n",
        "X_iris_tsne = tsne.fit_transform(X_iris)\n",
        "\n",
        "# Vizualizăm rezultatul\n",
        "plt.figure(figsize=(10, 7))\n",
        "scatter = plt.scatter(X_iris_tsne[:, 0], X_iris_tsne[:, 1], c=iris_labels, cmap='viridis', edgecolor='k', alpha=0.8)\n",
        "plt.title('Setul de date Iris vizualizat cu t-SNE')\n",
        "plt.xlabel('Componenta t-SNE 1')\n",
        "plt.ylabel('Componenta t-SNE 2')\n",
        "plt.legend(handles=scatter.legend_elements()[0], labels=list(df_iris['species'].unique()))\n",
        "plt.show()\n",
        "\n",
        "# OBS.: Observați cât de bine definite și separate sunt clusterele pentru\n",
        "# fiecare specie de iris. t-SNE excelează la acest tip de vizualizare, făcând\n",
        "# structura datelor foarte clară."
      ],
      "id": "G6KHxxbDDyOD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWboU3_CDyOD"
      },
      "outputs": [],
      "source": [
        "# __EXERCIȚIU__\n",
        "# Acum este rândul tău să aplici t-SNE pe setul de date 'wine'.\n",
        "# 1. Selectează caracteristicile (X_wine) și ținta (y_wine) din DataFrame-ul 'df_wine'.\n",
        "# 2. Aplică t-SNE pe X_wine pentru a reduce datele la 2 dimensiuni.\n",
        "# 3. Creează un scatter plot al rezultatului, colorând punctele după clasa de vin (y_wine).\n",
        "\n",
        "# 1. Datele sunt deja încărcate (X_wine, y_wine)\n",
        "\n",
        "# 2. Aplică t-SNE\n",
        "tsne_wine = TSNE(n_components=2, learning_rate='auto', init='random', random_state=42)\n",
        "X_wine_tsne = # ...\n",
        "\n",
        "# 3. Creează graficul\n",
        "plt.figure(figsize=(10, 7))\n",
        "# ...\n",
        "plt.title('Setul de date Wine vizualizat cu t-SNE')\n",
        "plt.xlabel('Componenta t-SNE 1')\n",
        "plt.ylabel('Componenta t-SNE 2')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# HINT: Poți folosi aceeași configurație pentru t-SNE ca în exemplul anterior,\n",
        "# inclusiv `random_state` pentru a obține un rezultat consistent."
      ],
      "id": "WWboU3_CDyOD"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}